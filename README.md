# Web-Scraping-Residente-Evil-Database-

## Resident Evil Database

Este projeto tem como objetivo criar um dataframe contendo informa√ß√µes detalhadas dos personagens de Resident Evil, obtidas por meio de raspagem de dados do site [Resident Evil Database](https://www.residentevildatabase.com/). As caracter√≠sticas extra√≠das incluem ano de nascimento, tipo sangu√≠neo, altura e peso de cada personagem.

## Etapas do Projeto

### 1. Coleta Inicial de Dados

Come√ßamos capturando a URL de um personagem em formato cURL, utilizando a ferramenta de desenvolvedor do navegador. Em seguida, convertendo o cURL para um script Python, realizamos a requisi√ß√£o simulando o comportamento do navegador.
![1](https://github.com/user-attachments/assets/da9629b8-4147-4e56-bdc5-e05cb807493c)
![2](https://github.com/user-attachments/assets/95608e41-745a-4cf5-9e4f-3b1fe79b801f)

O status code retornado foi 200, indicando que a requisi√ß√£o foi bem-sucedida. Inicialmente, obtivemos a p√°gina completa, mas nosso foco est√° nas caracter√≠sticas gerais do personagem.

### 2. Extra√ß√£o de Dados com Beautiful Soup

Utilizamos a biblioteca `Beautiful Soup` do Python para analisar o HTML da p√°gina e navegar na estrutura do documento de forma eficiente. Localizamos a `div` que cont√©m as informa√ß√µes desejadas e filtramos o par√°grafo espec√≠fico que cont√©m os detalhes do personagem.
![3](https://github.com/user-attachments/assets/4b146737-89e4-44ba-8aff-1238abb393ae)

```python
div_page = soup.find('div', class_='td-page-content')
paragraph = div_page.find_all('p')[1]
ems = paragraph.find_all('em')
```

As informa√ß√µes extra√≠das s√£o formatadas como:

```html
<em>Ano de nascimento: 1974 (24 anos em 1998)</em>
<em>Tipo sangu√≠neo: AB</em>
<em>Altura: Desconhecida.</em>
<em>Peso: Desconhecido.</em>
```
Com essas informa√ß√µes, podemos agora acessar o texto de cada elemento em:

```python
ems[0].text  # Retorna 'Ano de nascimento: 1974 (24 anos em 1998)'
```

### 3. Estrutura√ß√£o do C√≥digo em Fun√ß√µes


Para facilitar a coleta dos dados de todos os personagens, organizamos o c√≥digo em fun√ß√µes reutiliz√°veis. Primeiro, criamos uma fun√ß√£o para obter o conte√∫do de uma URL:


```python
def get_content(url):
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
    }
    response = requests.get(url, headers=headers)
    return response
```

Depois, criamos uma fun√ß√£o para extrair as informa√ß√µes b√°sicas de cada personagem:

```python
def get_basic_infos(soup):
    div_page = soup.find('div', class_='td-page-content')
    paragraph = div_page.find_all('p')[1]
    ems = paragraph.find_all('em')
    
    data = {}
    for i in ems:
        chave, valor = i.text.split(':')
        data[chave.strip()] = valor.strip()
    return data
```

A fun√ß√£o get_basic_infos √© projetada para extrair  informa√ß√µes espec√≠ficas dos personagens. Ela come√ßa localizando uma <div> com a classe 'td-page-content', que √© onde o conte√∫do principal da p√°gina est√° localizado. Dentro dessa <div>, a fun√ß√£o seleciona o segundo par√°grafo e, em seguida, busca por todas as tags <em> nesse par√°grafo.

Para cada tag <em>, a fun√ß√£o divide o texto usando ':' como separador. O texto antes dos ':' √© considerado a "chave" e o texto depois √© a "valor". Essas chaves e valores s√£o ent√£o armazenados em um dicion√°rio, com espa√ßos extras removidos.

Por fim, a fun√ß√£o retorna esse dicion√°rio, que cont√©m as informa√ß√µes extra√≠das e organizadas de forma estruturada.


### 4. Coleta de Dados de Todos os Personagens

Por fim, implementamos uma fun√ß√£o para coletar os links de todos os personagens no site e extrair as informa√ß√µes de cada um. Esses dados s√£o organizados em um dataframe e salvos em um arquivo Parquet ,pois suporta compress√£o de dados, o que significa que os arquivos ocupam menos espa√ßo em disco em compara√ß√£o com formatos como CSV ou JSON. Queremos uma formato mais elegante ü¶Ñ.

```python
def get_links():
    url = 'https://www.residentevildatabase.com/personagens/'
    resp = requests.get(url, headers=headers)
    soup_person = BeautifulSoup(resp.text, features="html.parser")
    ancoras = soup_person.find("div", class_="td-page-content").find_all("a")
    
    links = [i["href"] for i in ancoras]
    return links 
```
```python
links = get_links()
data = []

for i in tqdm(links):
    d = get_basic_infos(get_content(i))
    d["Link"] = i
    nome = i.strip("/").split("/")[-1].replace("-", " ").title()
    d["Nome"] = nome
    data.append(d)
```
O loop est√° iterando sobre uma lista de links de personagens acima, fazendo requisi√ß√µes web, extraindo dados, e processando essas informa√ß√µes. Dependendo do n√∫mero de links e da velocidade da rede, isso pode demorar, logo usamos o tqdm fornece uma barra de progresso visual no terminal ou notebook, permitindo que voc√™ veja quanto do trabalho j√° foi conclu√≠do e quanto ainda falta! Mesmo que esse projeto tenha um n√∫mero pequeno de dados.

```python
df = pd.DataFrame(data)
df.to_parquet("dados_re.parquet", index=False)

E pronto! Agora temos um dataframe completo com as informa√ß√µes dos personagens de Resident Evil, extra√≠do diretamente do site. O arquivo Parquet gerado pode ser utilizado para futuras an√°lises e visualiza√ß√µes.
